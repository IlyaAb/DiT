{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Diffusion Models with Transformer (DiT)\n",
    "\n",
    "This notebook samples from pre-trained DiT models. DiTs are class-conditional latent diffusion models trained on ImageNet that use transformers in place of U-Nets as the DDPM backbone. DiT outperforms all prior diffusion models on the ImageNet benchmarks.\n",
    "\n",
    "[Project Page](https://www.wpeebles.com/DiT) | [HuggingFace Space](https://huggingface.co/spaces/wpeebles/DiT) | [Paper](http://arxiv.org/abs/2212.09748) | [GitHub](github.com/facebookresearch/DiT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup / Установка нужных библиотек\n",
    "\n",
    "We recommend using GPUs (Runtime > Change runtime type > Hardware accelerator > GPU)\n",
    "\n",
    "Рекомендуется использовать GPU, в противном случае все будет ужасно медленным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the requirements file contains a list of libraries that I use on my home PC\n",
    "# в файле requirements представлен список библиотек, который используя я у себя на домашнем пк \n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Obtaining dependency information for torchinfo from https://files.pythonhosted.org/packages/72/25/973bd6128381951b23cdcd8a9870c6dcfc5606cb864df8eabd82e529f9c1/torchinfo-1.8.0-py3-none-any.whl.metadata\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#for collab \n",
    "\n",
    "!pip install diffusers==0.18.2\n",
    "!pip install accelerate\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Programming\\diffusion_transformer\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most of these libraries are needed only in case of experiments in a laptop, all important steps are done by accessing the .py files through the terminal, where the necessary imports are implemented\n",
    "#большинство из этих библиотек нужны только в случае экспериментов в ноутбуке, все важные шаги осуществляются путем обращения к .py файлам через терминал, где нужные импорты реалзиованы\n",
    "\n",
    "import DiT, os\n",
    "os.chdir(r'DiT')\n",
    "\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "import torchinfo\n",
    "from diffusion import create_diffusion \n",
    "from diffusers.models import AutoencoderKL\n",
    "from download import find_model  \n",
    "from models import DiT_XL_2 \n",
    "import shutil \n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"GPU not found. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DiT                                      294,912\n",
       "├─PatchEmbed: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       19,584\n",
       "│    └─Identity: 2-2                     --\n",
       "├─TimestepEmbedder: 1-2                  --\n",
       "│    └─Sequential: 2-3                   --\n",
       "│    │    └─Linear: 3-1                  296,064\n",
       "│    │    └─SiLU: 3-2                    --\n",
       "│    │    └─Linear: 3-3                  1,328,256\n",
       "├─LabelEmbedder: 1-3                     --\n",
       "│    └─Embedding: 2-4                    1,153,152\n",
       "├─ModuleList: 1-4                        --\n",
       "│    └─DiTBlock: 2-5                     --\n",
       "│    │    └─LayerNorm: 3-4               --\n",
       "│    │    └─Attention: 3-5               5,313,024\n",
       "│    │    └─LayerNorm: 3-6               --\n",
       "│    │    └─Mlp: 3-7                     10,622,592\n",
       "│    │    └─Sequential: 3-8              7,969,536\n",
       "│    └─DiTBlock: 2-6                     --\n",
       "│    │    └─LayerNorm: 3-9               --\n",
       "│    │    └─Attention: 3-10              5,313,024\n",
       "│    │    └─LayerNorm: 3-11              --\n",
       "│    │    └─Mlp: 3-12                    10,622,592\n",
       "│    │    └─Sequential: 3-13             7,969,536\n",
       "│    └─DiTBlock: 2-7                     --\n",
       "│    │    └─LayerNorm: 3-14              --\n",
       "│    │    └─Attention: 3-15              5,313,024\n",
       "│    │    └─LayerNorm: 3-16              --\n",
       "│    │    └─Mlp: 3-17                    10,622,592\n",
       "│    │    └─Sequential: 3-18             7,969,536\n",
       "│    └─DiTBlock: 2-8                     --\n",
       "│    │    └─LayerNorm: 3-19              --\n",
       "│    │    └─Attention: 3-20              5,313,024\n",
       "│    │    └─LayerNorm: 3-21              --\n",
       "│    │    └─Mlp: 3-22                    10,622,592\n",
       "│    │    └─Sequential: 3-23             7,969,536\n",
       "│    └─DiTBlock: 2-9                     --\n",
       "│    │    └─LayerNorm: 3-24              --\n",
       "│    │    └─Attention: 3-25              5,313,024\n",
       "│    │    └─LayerNorm: 3-26              --\n",
       "│    │    └─Mlp: 3-27                    10,622,592\n",
       "│    │    └─Sequential: 3-28             7,969,536\n",
       "│    └─DiTBlock: 2-10                    --\n",
       "│    │    └─LayerNorm: 3-29              --\n",
       "│    │    └─Attention: 3-30              5,313,024\n",
       "│    │    └─LayerNorm: 3-31              --\n",
       "│    │    └─Mlp: 3-32                    10,622,592\n",
       "│    │    └─Sequential: 3-33             7,969,536\n",
       "│    └─DiTBlock: 2-11                    --\n",
       "│    │    └─LayerNorm: 3-34              --\n",
       "│    │    └─Attention: 3-35              5,313,024\n",
       "│    │    └─LayerNorm: 3-36              --\n",
       "│    │    └─Mlp: 3-37                    10,622,592\n",
       "│    │    └─Sequential: 3-38             7,969,536\n",
       "│    └─DiTBlock: 2-12                    --\n",
       "│    │    └─LayerNorm: 3-39              --\n",
       "│    │    └─Attention: 3-40              5,313,024\n",
       "│    │    └─LayerNorm: 3-41              --\n",
       "│    │    └─Mlp: 3-42                    10,622,592\n",
       "│    │    └─Sequential: 3-43             7,969,536\n",
       "│    └─DiTBlock: 2-13                    --\n",
       "│    │    └─LayerNorm: 3-44              --\n",
       "│    │    └─Attention: 3-45              5,313,024\n",
       "│    │    └─LayerNorm: 3-46              --\n",
       "│    │    └─Mlp: 3-47                    10,622,592\n",
       "│    │    └─Sequential: 3-48             7,969,536\n",
       "│    └─DiTBlock: 2-14                    --\n",
       "│    │    └─LayerNorm: 3-49              --\n",
       "│    │    └─Attention: 3-50              5,313,024\n",
       "│    │    └─LayerNorm: 3-51              --\n",
       "│    │    └─Mlp: 3-52                    10,622,592\n",
       "│    │    └─Sequential: 3-53             7,969,536\n",
       "│    └─DiTBlock: 2-15                    --\n",
       "│    │    └─LayerNorm: 3-54              --\n",
       "│    │    └─Attention: 3-55              5,313,024\n",
       "│    │    └─LayerNorm: 3-56              --\n",
       "│    │    └─Mlp: 3-57                    10,622,592\n",
       "│    │    └─Sequential: 3-58             7,969,536\n",
       "│    └─DiTBlock: 2-16                    --\n",
       "│    │    └─LayerNorm: 3-59              --\n",
       "│    │    └─Attention: 3-60              5,313,024\n",
       "│    │    └─LayerNorm: 3-61              --\n",
       "│    │    └─Mlp: 3-62                    10,622,592\n",
       "│    │    └─Sequential: 3-63             7,969,536\n",
       "│    └─DiTBlock: 2-17                    --\n",
       "│    │    └─LayerNorm: 3-64              --\n",
       "│    │    └─Attention: 3-65              5,313,024\n",
       "│    │    └─LayerNorm: 3-66              --\n",
       "│    │    └─Mlp: 3-67                    10,622,592\n",
       "│    │    └─Sequential: 3-68             7,969,536\n",
       "│    └─DiTBlock: 2-18                    --\n",
       "│    │    └─LayerNorm: 3-69              --\n",
       "│    │    └─Attention: 3-70              5,313,024\n",
       "│    │    └─LayerNorm: 3-71              --\n",
       "│    │    └─Mlp: 3-72                    10,622,592\n",
       "│    │    └─Sequential: 3-73             7,969,536\n",
       "│    └─DiTBlock: 2-19                    --\n",
       "│    │    └─LayerNorm: 3-74              --\n",
       "│    │    └─Attention: 3-75              5,313,024\n",
       "│    │    └─LayerNorm: 3-76              --\n",
       "│    │    └─Mlp: 3-77                    10,622,592\n",
       "│    │    └─Sequential: 3-78             7,969,536\n",
       "│    └─DiTBlock: 2-20                    --\n",
       "│    │    └─LayerNorm: 3-79              --\n",
       "│    │    └─Attention: 3-80              5,313,024\n",
       "│    │    └─LayerNorm: 3-81              --\n",
       "│    │    └─Mlp: 3-82                    10,622,592\n",
       "│    │    └─Sequential: 3-83             7,969,536\n",
       "│    └─DiTBlock: 2-21                    --\n",
       "│    │    └─LayerNorm: 3-84              --\n",
       "│    │    └─Attention: 3-85              5,313,024\n",
       "│    │    └─LayerNorm: 3-86              --\n",
       "│    │    └─Mlp: 3-87                    10,622,592\n",
       "│    │    └─Sequential: 3-88             7,969,536\n",
       "│    └─DiTBlock: 2-22                    --\n",
       "│    │    └─LayerNorm: 3-89              --\n",
       "│    │    └─Attention: 3-90              5,313,024\n",
       "│    │    └─LayerNorm: 3-91              --\n",
       "│    │    └─Mlp: 3-92                    10,622,592\n",
       "│    │    └─Sequential: 3-93             7,969,536\n",
       "│    └─DiTBlock: 2-23                    --\n",
       "│    │    └─LayerNorm: 3-94              --\n",
       "│    │    └─Attention: 3-95              5,313,024\n",
       "│    │    └─LayerNorm: 3-96              --\n",
       "│    │    └─Mlp: 3-97                    10,622,592\n",
       "│    │    └─Sequential: 3-98             7,969,536\n",
       "│    └─DiTBlock: 2-24                    --\n",
       "│    │    └─LayerNorm: 3-99              --\n",
       "│    │    └─Attention: 3-100             5,313,024\n",
       "│    │    └─LayerNorm: 3-101             --\n",
       "│    │    └─Mlp: 3-102                   10,622,592\n",
       "│    │    └─Sequential: 3-103            7,969,536\n",
       "│    └─DiTBlock: 2-25                    --\n",
       "│    │    └─LayerNorm: 3-104             --\n",
       "│    │    └─Attention: 3-105             5,313,024\n",
       "│    │    └─LayerNorm: 3-106             --\n",
       "│    │    └─Mlp: 3-107                   10,622,592\n",
       "│    │    └─Sequential: 3-108            7,969,536\n",
       "│    └─DiTBlock: 2-26                    --\n",
       "│    │    └─LayerNorm: 3-109             --\n",
       "│    │    └─Attention: 3-110             5,313,024\n",
       "│    │    └─LayerNorm: 3-111             --\n",
       "│    │    └─Mlp: 3-112                   10,622,592\n",
       "│    │    └─Sequential: 3-113            7,969,536\n",
       "│    └─DiTBlock: 2-27                    --\n",
       "│    │    └─LayerNorm: 3-114             --\n",
       "│    │    └─Attention: 3-115             5,313,024\n",
       "│    │    └─LayerNorm: 3-116             --\n",
       "│    │    └─Mlp: 3-117                   10,622,592\n",
       "│    │    └─Sequential: 3-118            7,969,536\n",
       "│    └─DiTBlock: 2-28                    --\n",
       "│    │    └─LayerNorm: 3-119             --\n",
       "│    │    └─Attention: 3-120             5,313,024\n",
       "│    │    └─LayerNorm: 3-121             --\n",
       "│    │    └─Mlp: 3-122                   10,622,592\n",
       "│    │    └─Sequential: 3-123            7,969,536\n",
       "│    └─DiTBlock: 2-29                    --\n",
       "│    │    └─LayerNorm: 3-124             --\n",
       "│    │    └─Attention: 3-125             5,313,024\n",
       "│    │    └─LayerNorm: 3-126             --\n",
       "│    │    └─Mlp: 3-127                   10,622,592\n",
       "│    │    └─Sequential: 3-128            7,969,536\n",
       "│    └─DiTBlock: 2-30                    --\n",
       "│    │    └─LayerNorm: 3-129             --\n",
       "│    │    └─Attention: 3-130             5,313,024\n",
       "│    │    └─LayerNorm: 3-131             --\n",
       "│    │    └─Mlp: 3-132                   10,622,592\n",
       "│    │    └─Sequential: 3-133            7,969,536\n",
       "│    └─DiTBlock: 2-31                    --\n",
       "│    │    └─LayerNorm: 3-134             --\n",
       "│    │    └─Attention: 3-135             5,313,024\n",
       "│    │    └─LayerNorm: 3-136             --\n",
       "│    │    └─Mlp: 3-137                   10,622,592\n",
       "│    │    └─Sequential: 3-138            7,969,536\n",
       "│    └─DiTBlock: 2-32                    --\n",
       "│    │    └─LayerNorm: 3-139             --\n",
       "│    │    └─Attention: 3-140             5,313,024\n",
       "│    │    └─LayerNorm: 3-141             --\n",
       "│    │    └─Mlp: 3-142                   10,622,592\n",
       "│    │    └─Sequential: 3-143            7,969,536\n",
       "├─FinalLayer: 1-5                        --\n",
       "│    └─LayerNorm: 2-33                   --\n",
       "│    └─Linear: 2-34                      36,896\n",
       "│    └─Sequential: 2-35                  --\n",
       "│    │    └─SiLU: 3-144                  --\n",
       "│    │    └─Linear: 3-145                2,656,512\n",
       "=================================================================\n",
       "Total params: 675,129,632\n",
       "Trainable params: 674,834,720\n",
       "Non-trainable params: 294,912\n",
       "================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show model arch. how much params\n",
    "\n",
    "image_size = 256\n",
    "latent_size = int(image_size) // 8\n",
    "model = DiT_XL_2(input_size=latent_size)\n",
    "\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading the dataset. Here is an example with an already existing drone dataset from /\n",
    "# Загрузка датасета. Здесь представлен пример с уже имеющимся датасетом дронов из\n",
    "\n",
    "https://www.kaggle.com/datasets/dasmehdixtr/drone-dataset-uav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving only pictures in a separate folder, when creating a folder, you need to remember about the hierarchy, each class has a separate folder\n",
    "#сохранение в отдельную папку только картинок, при создании папки нужно помнить об иерархии, каждому классу отдельная папка \n",
    "source = '...drone_dataset_yolo\\dataset_txt'\n",
    "for i in os.listdir(source):\n",
    "    if 'jpg' in i:\n",
    "        shutil.copy(f'{source}\\{i}', '...\\drones_for_dit\\drones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extracting features from dataset /\n",
    "# Извлечение фич из данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание фич с помощью энкодера из AutoencoderKL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae arch\n",
    "vae_model = \"stabilityai/sd-vae-ft-ema\"\n",
    "vae = AutoencoderKL.from_pretrained(vae_model) #.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "AutoencoderKL                                      --\n",
       "├─Encoder: 1-1                                     --\n",
       "│    └─Conv2d: 2-1                                 3,584\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─DownEncoderBlock2D: 3-1                738,944\n",
       "│    │    └─DownEncoderBlock2D: 3-2                2,690,304\n",
       "│    │    └─DownEncoderBlock2D: 3-3                10,754,560\n",
       "│    │    └─DownEncoderBlock2D: 3-4                9,443,328\n",
       "│    └─UNetMidBlock2D: 2-3                         --\n",
       "│    │    └─ModuleList: 3-5                        1,051,648\n",
       "│    │    └─ModuleList: 3-6                        9,443,328\n",
       "│    └─GroupNorm: 2-4                              1,024\n",
       "│    └─SiLU: 2-5                                   --\n",
       "│    └─Conv2d: 2-6                                 36,872\n",
       "├─Decoder: 1-2                                     --\n",
       "│    └─Conv2d: 2-7                                 18,944\n",
       "│    └─ModuleList: 2-8                             --\n",
       "│    │    └─UpDecoderBlock2D: 3-7                  16,524,800\n",
       "│    │    └─UpDecoderBlock2D: 3-8                  16,524,800\n",
       "│    │    └─UpDecoderBlock2D: 3-9                  4,855,296\n",
       "│    │    └─UpDecoderBlock2D: 3-10                 1,067,648\n",
       "│    └─UNetMidBlock2D: 2-9                         --\n",
       "│    │    └─ModuleList: 3-11                       1,051,648\n",
       "│    │    └─ModuleList: 3-12                       9,443,328\n",
       "│    └─GroupNorm: 2-10                             256\n",
       "│    └─SiLU: 2-11                                  --\n",
       "│    └─Conv2d: 2-12                                3,459\n",
       "├─Conv2d: 1-3                                      72\n",
       "├─Conv2d: 1-4                                      20\n",
       "===========================================================================\n",
       "Total params: 83,653,863\n",
       "Trainable params: 83,653,863\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this part of the model is not trained, first the encoder is taken to extract the features, and at the end the decoder is taken to get the pictures again\n",
    "# это часть модели не обучается, сначалаб берется жнкодер для извленчение фич, а в конце берется декодер, чтобы снова получить картинки \n",
    "torchinfo.summary(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun \\\n",
    "--nnodes=1 \\\n",
    "--nproc_per_node=1 extract_features.py \\\n",
    "--data-path ...\\drones_for_dit \\\n",
    "--features-path ...\\dit_features2 \\\n",
    "--image-size 256 \\\n",
    "--global-batch-size 256 \\\n",
    "--global-seed 0 \\\n",
    "--vae ema \\\n",
    "--num-workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Training the model on the received features from the encoder\n",
    "# Тренировка модели на полученных фичах из энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch \\\n",
    "--mixed_precision fp16 train.py \\\n",
    "--feature-path ...\\dit_features \\\n",
    "--results-dir ...\\dit_weights \\\n",
    "--model DiT-S/2 \\\n",
    "--image-size 256 \\\n",
    "--num-classes 1 \\\n",
    "--epochs=10 \\\n",
    "--global-batch-size=16 \\\n",
    "--global-seed 0 \\\n",
    "--num-workers 0 \\\n",
    "--log-every=25 \\\n",
    "--ckpt-every=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling several imgs\n",
    "\n",
    "!python sample.py \\\n",
    "--model DiT-XL/2 \\\n",
    "--vae ema \\\n",
    "--image-size=256 \\\n",
    "--num-classes=1000 \\\n",
    "--cfg-scale 8 \\\n",
    "--num-sampling-steps=256 \\\n",
    "--seed 0 \\\n",
    "--ckpt ...\\DiT-XL-2-256x256.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling many imgs, very useful if u have mote than one gpu\n",
    "\n",
    "!torchrun sample_ddp.py \\\n",
    "--model DiT-XL/2 \\\n",
    "--vae ema \\\n",
    "--sample-dir /path/to/generated/data \\\n",
    "--per-proc-batch-size 32 \\\n",
    "--num-fid-sample 400 \\\n",
    "--image-size 265 \\\n",
    "--num-classes 1 \\\n",
    "--cfg-scale 7 \\\n",
    "--num-sampling-steps 250 \\\n",
    "--global-seed 42 \\\n",
    "--ckpt /path/to/your/trained/models/weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
